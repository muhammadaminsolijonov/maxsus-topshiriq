# -*- coding: utf-8 -*-
"""Untitled13.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/github/asis012/3D-/blob/master/QNLI.ipynb
"""

import sys, os
from pathlib import Path
if not "cwd" in globals():
    cwd = Path(os.getcwd())
cwd

!pip install git+git://github.com/HazyResearch/snorkel.git@snorkel-superglue

cd tmp

ls

pwd

cd ..

ls

cd ..

ls

cd root

cd ..

cd usr

cd ..



ls



pwd



from snorkel.mtl.data import MultitaskDataLoader
from snorkel.mtl.model import MultitaskModel
from snorkel.mtl.snorkel_config import default_config as config
from snorkel.mtl.trainer import Trainer

import superglue_tasks
from tokenizer import get_tokenizer
from utils import task_dataset_to_dataframe

from dataloaders import get_jsonl_path
from superglue_parsers.wic import get_rows

!unzip WiC.zip

ls

task_name= "WiC"
data_dir = os.environ.get("SUPERGLUEDATA", os.path.join(str(cwd.parents[0]), "data"))
split = "valid"

data_df['sentence1_idx'] = ""

# Observe the result

data_df['sentence2_idx'] = ""

data_df

task_name = "WiC"
data_dir = os.environ.get("SUPERGLUEDATA", os.path.join(str(cwd.parents[0]), "./snorkel-superglue-master/Data"))
split = "valid"
max_data_samples = None # max examples to include in dataset

jsonl_path = get_jsonl_path(data_dir, task_name, split)
# print(jsonl_path)
# data_df = pd.read_json(jsonl_path, lines=True)

wic_df = pd.DataFrame.from_records(get_rowss(jsonl_path, max_data_samples))

wic_df[["sentence1_idx"]].head()

import json


from pytorch_pretrained_bert import BertTokenizer


import superglue_parsers

def get_rowss(jsonl_path, max_data_samples):

  logger.info(f"Loading data from {jsonl_path}.")

  rows = [json.loads(row) for row in open(jsonl_path, encoding="utf-8")]


  print(rows)

  for i in range(2):
    logger.info(f"Sample {i}: {rows[i]}")
#     print(open(jsonl_path, encoding="utf-8"))
    # Truncate to max_data_samples


  if max_data_samples:
    rows = rows[:max_data_samples]
    logger.info(f"Truncating to {max_data_samples} samples.")

  for row in rows:
    row["sentence1_idx"] = int(row["idx"])
    row["sentence2_idx"] = int(row["idx"])
    row["label"] = row["label"] if "label" in row else True

  return rows
import logging
logger = logging.getLogger(__name__)

def get_ro(jsonl_path, max_data_samples):
    logger.info(f"Loading data from {jsonl_path}.")
    rows = [json.loads(row) for row in open(jsonl_path, encoding="utf-8")]
    for i in range(2):
        logger.info(f"Sample {i}: {rows[i]}")
    print(rows)
    # Truncate to max_data_samples
    if max_data_samples:
        rows = rows[:max_data_samples]
        logger.info(f"Truncating to {max_data_samples} samples.")

    for row in rows:
        row["sentence1_idx"] = int(row["idx"])
        row["sentence2_idx"] = int(row["idx"])
        row["label"] = row["label"] if "label" in row else True

    return rows

bert_model = "bert-large-cased"
tokenizer_name = "bert-large-cased"
batch_size = 4
max_sequence_length = 256

tokenizer = get_tokenizer(tokenizer_name)
print(max_data_samples)

from dataloaders import get_dataloaders

dataloaders = get_dataloaders(
    task_name=TASK_NAME,
    tokenizer_name=BERT_MODEL,
    **dataloader_config
)

def get_dataseet(
    data_dir: str,
    task_name: str,
    split: str,
    tokenizer: BertTokenizer,
    max_data_samples: int,
    max_sequence_length: int,
):
    jsonl_path = get_jsonl_path(data_dir, task_name, split)
    return superglue_parsers.parser[task_name](
        jsonl_path, tokenizer, max_data_samples, max_sequence_length
    )

base_task = superglue_tasks.task_funcs[task_name](bert_model)
tasks = [base_task]
tasks

!rm wic.py

!unzip RTE.zip

from google.colab import files
uploaded = files.upload()

"""When you execute your script, CWD is set to the directory where your script is."""

import os, sys
from pathlib import Path

if not "cwd" in globals():
    cwd = Path(os.getcwd())

"""Usine Bert pre trained model.
Out of four model using bert_large-cases


Use ADAM optimer defined by bert
"""

TASK_NAME = "RTE"
BERT_MODEL = "bert-large-cased"

dataloader_config = {
    "batch_size": 8,
    "data_dir": os.environ.get("SUPERGLUEDATA", os.path.join(str(cwd.parents[0]), "./snorkel-superglue-master/Data")),
    "splits": ["train", "valid"],
}

trainer_config = {
    "lr": 2e-5,
    "optimizer": "adamax",
    "n_epochs": 15,
    "conter_unit": "epochs",
    "evaluation_freq": 0.25,
}

"""load data and return dataloaders"""

from dataloaders import get_dataloaders

dataloaders = get_dataloaders(
    task_name=TASK_NAME,
    tokenizer_name=BERT_MODEL,
    **dataloader_config
)

dataloaders

from superglue_tasks import task_funcs

task = task_funcs[TASK_NAME](BERT_MODEL)

from snorkel.mtl.model import MultitaskModel
from snorkel.mtl.trainer import Trainer

"""get the pre trained bert  model which will save our time and memory
  if you want to train the bert model:
  use following code
  trainer = Trainer(**trainer_config)

  trainer.train_model(model, dataloaders)
"""

! wget -nc https://www.dropbox.com/s/t2ri9o0iz765hsn/RTE_bert.pth?dl=0 && mv RTE_bert.pth?dl=0 RTE_bert.pth
model.load("RTE_bert.pth")

dataloaders[1]

# Commented out IPython magic to ensure Python compatibility.
# %%time
# model.score(dataloaders[1])

"""Here we have found the accuracy of bert model. which is 0.736.

Now lets use some slicing functions
"""

from snorkel.slicing.sf import slicing_function

@slicing_function()
def slice_temporal_preposition(example):
    temporal_prepositions = ["after", "before", "past"]
    both_sentences = example.sentence1 + example.sentence2
    return any([p in both_sentences for p in temporal_prepositions])

@slicing_function()
def slice_possessive_preposition(example):
    possessive_prepositions = ["inside of", "with", "within"]
    both_sentences = example.sentence1 + example.sentence2
    return any([p in both_sentences for p in possessive_prepositions])

!pip install pytorch_pretrained_bert

!pip install allennlp

!pip install jsonlines

!pip install git+git://github.com/HazyResearch/snorkel.git@snorkel-superglue

!pip install git+https://github.com/HazyResearch/snorkel

print("hello")

# Commented out IPython magic to ensure Python compatibility.
# %time

!pip install git+git://github.com/HazyResearch/snorkel.git@snorkel-superglue

from snorkel.mtl.model import MultitaskModel
from snorkel.mtl.trainer import Trainer

!pip install snorkel-metal

!unzip snorkel-superglue-master.zip

import os, sys
import pandas as pd
from pathlib import Path
import json
if not "cwd" in globals():
    cwd = Path(os.getcwd())

TASK_NAME = "QNLI"
BERT_MODEL = "bert-large-cased"
split = "valid"

dataloader_config = {
    "batch_size": 100,
    "data_dir": "Data",
    "splits": ["train", "valid"],
}

trainer_config = {
    "lr": 0.6,
    "optimizer": "adamax",
    "n_epochs": 5,
    "conter_unit": "epochs",
    "evaluation_freq": 0.5,
}

cd snorkel-superglue-master/t

ls

from dataloaders import get_jsonl_path
from superglue_parsers.wic import get_rows
max_data_samples =1120

cd ..

from dataloaders import get_jsonl_path
from superglue_parsers.wic import get_rows
max_data_samples =1120

from dataloaders import get_dataloaders

dataloaders = get_dataloaders(
    task_name=TASK_NAME,
    tokenizer_name=BERT_MODEL,
    **dataloader_config
)

from superglue_tasks import task_funcs

task = task_funcs[TASK_NAME](BERT_MODEL)

from snorkel.mtl.model import MultitaskModel
from snorkel.mtl.trainer import Trainer

model = MultitaskModel(tasks=[task], device=-1, dataparallel=False)

model.load("tutorials/model_with_qnli.pth")

# Commented out IPython magic to ensure Python compatibility.
# %%time
# model.score(dataloaders[1])

